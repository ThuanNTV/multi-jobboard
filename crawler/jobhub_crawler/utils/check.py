import os
import json
import logging
from pathlib import Path
from datetime import datetime

from typing import List, Dict, Union, Optional, Any

from jobhub_crawler.utils.helpers import _find_folder, _find_latest_file, _find_project_root

project_root = _find_project_root(Path(__file__))
crawler_folder = _find_folder('crawler', search_dir=project_root)
output_folder = _find_folder('output', search_dir=crawler_folder)
last_file_output = _find_latest_file(search_dir=output_folder, suffix='.json')


def _check_valid_input(data_check: Any) -> bool:
    """Ki·ªÉm tra ƒë·∫ßu v√†o c√≥ ph·∫£i list v√† kh√¥ng r·ªóng kh√¥ng."""
    return isinstance(data_check, list) and bool(data_check)

def _open_and_read_file(file_path: str, key_level1: str, key_level2: str) -> Union[List[Dict[str, str]], Dict]:
    """
    ƒê·ªçc file JSON v√† tr√≠ch xu·∫•t d·ªØ li·ªáu theo key_level1 v√† key_level2.

    Args:
        file_path (str): ƒê∆∞·ªùng d·∫´n t·ªõi file JSON.
        key_level1 (str): T√™n key c·∫•p 1 ('jobs' ho·∫∑c 'metadata').
        key_level2 (str): T√™n key c·∫•p 2 c·∫ßn tr√≠ch xu·∫•t trong 'jobs' (v√≠ d·ª• 'url').

    Returns:
        N·∫øu key_level1 == 'jobs': List c√°c dict {'title': ..., 'url': ...}
        N·∫øu key_level1 == 'metadata': Dict metadata ƒë·∫ßy ƒë·ªß.
        N·∫øu l·ªói ho·∫∑c kh√¥ng t√¨m th·∫•y: tr·∫£ v·ªÅ r·ªóng ([] ho·∫∑c {}).
    """
    if not isinstance(file_path, str) or not os.path.isfile(file_path):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i ho·∫∑c t√™n file kh√¥ng h·ª£p l·ªá: {file_path}")
        return [] if key_level1 == 'jobs' else {}

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            json_data = json.load(f)

        if not isinstance(json_data, dict):
            print("‚ùå File JSON kh√¥ng ch·ª©a ƒë·ªëi t∆∞·ª£ng c·∫•p cao nh·∫•t d·∫°ng dict.")
            return [] if key_level1 == 'jobs' else {}

        if key_level1 == 'jobs':
            if key_level1 not in json_data:
                print(f"‚ùå Key '{key_level1}' kh√¥ng t·ªìn t·∫°i trong file JSON.")
                return []
            extracted_values = []
            for entry in json_data[key_level1]:
                if isinstance(entry, dict) and key_level2 in entry:
                    extracted_values.append({
                        'title': entry.get('title', ''),
                        'url': entry[key_level2]
                    })
                else:
                    print(f"‚ö†Ô∏è M·ªôt ph·∫ßn t·ª≠ kh√¥ng c√≥ key '{key_level2}' ho·∫∑c kh√¥ng ph·∫£i dict: {entry}")
            return extracted_values

        elif key_level1 == 'metadata':
            if key_level1 not in json_data:
                print(f"‚ùå Key '{key_level1}' kh√¥ng t·ªìn t·∫°i trong file JSON.")
                return {}
            return json_data[key_level1]

        else:
            print(f"‚ùå Key c·∫•p 1 '{key_level1}' ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£.")
            return [] if key_level1 == 'jobs' else {}

    except json.JSONDecodeError as e:
        print(f"‚ùå L·ªói gi·∫£i m√£ JSON: {e}")
        return [] if key_level1 == 'jobs' else {}
    except Exception as e:
        print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi ƒë·ªçc file: {e}")
        return [] if key_level1 == 'jobs' else {}


def _find_diff_text_in_array(data: List[str], data_check: List[str]) -> List[str]:
    """Tr·∫£ v·ªÅ c√°c URL kh√°c bi·ªát gi·ªØa hai danh s√°ch."""
    set_data = set(data)
    set_data_check = set(data_check)

    only_in_data = set_data - set_data_check
    only_in_data_check = set_data_check - set_data

    return list(only_in_data.union(only_in_data_check))

def _find_diff_dict(data: List[Dict], data_check: List[Dict]) -> List[Dict]:
    """
    Tr·∫£ v·ªÅ c√°c dict trong data_check c√≥ URL ch∆∞a t·ªìn t·∫°i trong data.
    """
    # T·∫≠p h·ª£p c√°c URL ƒë√£ t·ªìn t·∫°i trong data
    existing_urls = {item["url"] for item in data if "url" in item}

    # Tr·∫£ v·ªÅ c√°c dict trong data_check c√≥ URL ch∆∞a t·ª´ng xu·∫•t hi·ªán trong data
    result = [
        item for item in data_check
        if "url" in item and item["url"] not in existing_urls
    ]

    return result

def _find_diff_dict_2(data: List[Dict], data_check: List[Dict]) -> List[Dict]:
    """Tr·∫£ v·ªÅ c√°c dict c√≥ URL kh√°c bi·ªát gi·ªØa hai danh s√°ch."""
    # L·ªçc v√† √°nh x·∫° url -> dict
    data_urls = {item["url"]: item for item in data if "url" in item}
    check_urls = {item["url"]: item for item in data_check if "url" in item}

    # T·∫≠p h·ª£p c√°c url ch·ªâ xu·∫•t hi·ªán ·ªü m·ªôt trong hai danh s√°ch
    diff_urls = set(data_urls) ^ set(check_urls)  # symmetric_difference

    # Thu th·∫≠p dict t∆∞∆°ng ·ª©ng v·ªõi url kh√°c bi·ªát
    result = [data_urls.get(url) or check_urls.get(url) for url in diff_urls]

    return result


def _get_data_in_file(
        file_path: Optional[str] = None,
        key_level1: str = "jobs",
        key_level2: str = "url"
) -> Optional[List[str]]:
    """
    ƒê·ªçc v√† tr√≠ch xu·∫•t danh s√°ch URL t·ª´ file JSON.

    Args:
        file_path (Optional[str]): File c·∫ßn ƒë·ªçc. N·∫øu kh√¥ng truy·ªÅn, m·∫∑c ƒë·ªãnh d√πng `last_file_output`.
        key_level1 (str): Key c·∫•p 1.
        key_level2 (str): Key c·∫•p 2.

    Returns:
        Optional[List[str]]: Danh s√°ch URL n·∫øu th√†nh c√¥ng, None n·∫øu l·ªói.
    """
    target_file = file_path or last_file_output

    if not target_file:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file JSON ph√π h·ª£p.")
        return None

    data = _open_and_read_file(file_path=target_file, key_level1=key_level1, key_level2=key_level2)

    if len(data) > 1:
        if not _check_valid_input(data):
            print("‚ùå D·ªØ li·ªáu ƒë·ªçc ƒë∆∞·ª£c kh√¥ng h·ª£p l·ªá.")
            return None

    return data

def _merge_two_records(record2: Dict[str, Any], filename: Optional[str] = None) -> Optional[str]:
    try:
        input_path = Path(last_file_output)
        if not input_path.exists():
            logging.warning(f"[‚ö†Ô∏è] File g·ªëc kh√¥ng t·ªìn t·∫°i: {input_path}")
            return None

        with open(input_path, "r", encoding="utf-8") as file:
            record1 = json.load(file)

    except Exception as e:
        logging.error(f"[‚ùå] L·ªói khi ƒë·ªçc file ƒë·∫ßu v√†o {last_file_output}: {e}")
        return None

    try:
        # G·ªôp job v√† lo·∫°i tr√πng theo URL
        job_map = {
            job.get("url"): job
            for job in record1.get("jobs", []) + record2.get("jobs", [])
            if job.get("url")
        }
        jobs = list(job_map.values())

        # T·ªïng h·ª£p source
        sources = {}
        for job in jobs:
            src = job.get("source")
            if src:
                sources[src] = sources.get(src, 0) + 1

        # Metadata
        meta1 = record1.get("metadata", {})
        meta2 = record2.get("metadata", {})
        created_at = max(
            meta1.get("created_at", "1970-01-01"),
            meta2.get("created_at", "1970-01-01"),
            key=lambda x: datetime.fromisoformat(x)
        )
        execution_time = meta1.get("execution_time", 0.0) + meta2.get("execution_time", 0.0)

        # Output record
        output = {
            "metadata": {
                "total_jobs": len(jobs),
                "created_at": created_at,
                "execution_time": execution_time,
                "sources": sources
            },
            "jobs": jobs
        }

        # T·∫°o t√™n file n·∫øu ch∆∞a c√≥
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"jobs_{timestamp}.json"

        filepath = os.path.join(output_folder, filename)
        output_path = Path(filepath)

        # Ghi file m·ªõi
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
            f.flush()
            os.fsync(f.fileno())

        # Sau khi ƒë√£ merge v√† ghi file th√†nh c√¥ng, m·ªõi xo√° b·∫£n c≈©
        try:
            input_path.unlink()
            logging.info(f"[üóëÔ∏è] ƒê√£ xo√° file g·ªëc: {input_path}")
        except Exception as del_err:
            logging.warning(f"[‚ö†Ô∏è] Kh√¥ng th·ªÉ xo√° file g·ªëc: {input_path} - {del_err}")

        logging.info(f"[‚úÖ] G·ªôp file th√†nh c√¥ng: {output_path}")
        return str(output_path)

    except Exception as e:
        logging.error(f"[‚ùå] L·ªói khi merge records: {e}")
        return None


# if __name__ == '__main__':
#     data = get_data_in_file()
#     if data:
#         print(f"‚úÖ Tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(data)} URL:")
#         for url in data:
#             print("   ‚Ä¢", url)
#
#     print(find_diff_urls(data, data_check))
