import time
import requests
import threading
import undetected_chromedriver as uc

from threading import Lock
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from selenium.webdriver.common.by import By
from concurrent.futures import ThreadPoolExecutor, as_completed
from jobhub_crawler.core.job_item import JobItem
from jobhub_crawler.core.base_crawler import BaseCrawler
from jobhub_crawler.utils.notifier import _send_telegram_message
from jobhub_crawler.utils.check import _get_data_in_file, _find_diff_dict
from jobhub_crawler.utils.helpers import _get_total_page, _chunk_pages, _wait_for_element_with_driver, \
    _remove_duplicates


class ChromeDriverPool:
    def __init__(self, headless=True):
        self.lock = Lock()
        self.headless = headless

    def get_driver(self):
        with self.lock:
            options = uc.ChromeOptions()
            if self.headless:
                options.add_argument('--headless=new')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument("--disable-gpu")
            options.add_argument("--disable-software-rasterizer")
            options.add_argument("--disable-dev-shm-usage")
            # options.add_argument("--single-process")
            return uc.Chrome(options=options, use_subprocess=False)


class NewItViecSpider(BaseCrawler):
    '''Tr√¨nh thu th·∫≠p (Spider) danh s√°ch vi·ªác l√†m t·ª´ ItViec.com '''

    def __init__(self, headless=False, max_workers=2, use_undetected=True):
        """
        Kh·ªüi t·∫°o spider ItViec v·ªõi kh·∫£ nƒÉng v∆∞·ª£t qua b·∫£o m·∫≠t Cloudflare

        Tham s·ªë:
            headless (bool): Ch·∫°y ·ªü ch·∫ø ƒë·ªô kh√¥ng hi·ªÉn th·ªã tr√¨nh duy·ªát n·∫øu l√† True
            max_workers (int): S·ªë l∆∞·ª£ng lu·ªìng x·ª≠ l√Ω song song t·ªëi ƒëa
            use_undetected (bool): S·ª≠ d·ª•ng undetected_chromedriver ƒë·ªÉ v∆∞·ª£t qua Cloudflare

        """
        # Lu√¥n g·ªçi h√†m kh·ªüi t·∫°o c·ªßa l·ªõp cha tr∆∞·ªõc
        ua = UserAgent()
        super().__init__(headless=headless,
                         user_agent=ua,
                         use_undetected=True
                         )
        self.headless = headless
        self.use_undetected = use_undetected
        self.max_workers = max_workers

        # N·∫øu s·ª≠ d·ª•ng undetected_chromedriver, h√£y thay th·∫ø tr√¨nh ƒëi·ªÅu khi·ªÉn (driver) th√¥ng th∆∞·ªùng.
        if use_undetected:
            # ƒê√≥ng tr√¨nh ƒëi·ªÅu khi·ªÉn m·∫∑c ƒë·ªãnh n·∫øu n√≥ ƒëang t·ªìn t·∫°i.
            if hasattr(self, 'driver') and self.driver:
                try:
                    self.driver.quit()
                except Exception as e:
                    self.logger.warning(f"Error closing default driver: {str(e)}")

            # Kh·ªüi t·∫°o tr√¨nh ƒëi·ªÅu khi·ªÉn ·∫©n danh (undetected driver).
            BaseCrawler._init_undetected_driver(self)

        self.jobs = []
        self.urls = []
        self.error_count = 0
        self.lock = threading.Lock()
        self.base_url = "https://itviec.com/it-jobs"
        self.session = requests.Session()
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Referer": "https://www.google.com/",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "cross-site",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1",
            "DNT": "1",
            "TE": "trailers"
        }

        self.driver_pool = ChromeDriverPool(headless=self.headless)  # ‚úÖ TH√äM D√íNG N√ÄY
        self.session.headers.update(self.headers)

    def run(self):
        '''Th·ª±c thi tr√¨nh thu th·∫≠p ƒë·ªÉ l·∫•y danh s√°ch vi·ªác l√†m t·ª´ ItViec, ƒëa lu·ªìng v∆∞·ª£t Cloudflare.'''
        self.logger.info(f'üöÄ Starting ItViec crawler with {self.max_workers} threads...')
        _send_telegram_message('', f'Starting ItViec crawler with {self.max_workers} threads!', '', '', '')

        self.get(self.base_url)

        if 'itviec' not in self.driver.current_url:
            self.logger.error(f'‚ùå L·ªói khi truy c·∫≠p trang web {self.driver.current_url}')
            return []

        # L·∫•y cookies t·ª´ Selenium -> ƒë·ªÉ chia s·∫ª cho c√°c requests sau n√†y (n·∫øu c·∫ßn)
        selenium_cookies = self.driver.get_cookies()
        for cookie in selenium_cookies:
            self.session.cookies.set(cookie['name'], cookie['value'])

        # total_pages = _get_total_page(self, '//div[@class="page" or contains(@class, "pagination")][last()]')
        total_pages = 5

        self.quit()

        page_ranges = _chunk_pages(self, total_pages, self.max_workers)

        job_urls = self._result_crawl_url(page_ranges)
        old_url = _get_data_in_file()
        new_urls = _find_diff_dict(old_url, job_urls)
        if new_urls and len(new_urls) >= 1:
            self.logger.info(f"Fetching descriptions for {len(new_urls)} jobs using {self.max_workers} threads")

            # Danh s√°ch ƒë·ªÉ l∆∞u c√°c URL c√≥ l·ªói
            failed_urls = []

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {
                    executor.submit(self._fetch_job_description_with_retry, url, retries=3, delay=2): url for url in
                    new_urls
                }

                for future in as_completed(future_to_url):
                    url_obj = future_to_url[future]
                    try:
                        result = future.result()
                        if result:
                            self.jobs.append(result)
                    except Exception as e:
                        self.logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {url_obj['url']}: {str(e)}")
                        failed_urls.append(url_obj)  # Th√™m v√†o danh s√°ch failed URLs
                        self.error_count += 1
            if self.error_count >= 1:
                _send_telegram_message('', f'Finished crawling ItViec. Collected {len(self.jobs)} job descriptions!', '', '',
                                   f'{self.error_count}')

            # Th·ª≠ l·∫°i c√°c URL b·ªã l·ªói n·∫øu c√≥
            if failed_urls:
                _send_telegram_message('', f'th·ª≠ l·∫•y l·∫°i d·ªØ li·ªáu c·ªßa {len(failed_urls)} job descriptions!', '', '',
                                       f'{self.error_count}')
                failed_urls = _remove_duplicates(failed_urls)
                self.error_count = 0
                self.logger.info(f"Retrying {len(failed_urls)} failed URLs...")
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_url = {
                        executor.submit(self._fetch_job_description_with_retry, url, retries=3, delay=2): url for url in
                        failed_urls
                    }

                    for future in as_completed(future_to_url):
                        url_obj = future_to_url[future]
                        try:
                            result = future.result()
                            if result:
                                self.jobs.append(result)
                        except Exception as e:
                            self.logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω l·∫°i {url_obj['url']}: {str(e)}")
                            self.error_count += 1

            self.logger.info(f"Finished crawling. Collected {len(self.jobs)} job descriptions.")
            if self.error_count >= 1:
                _send_telegram_message('', f'Finished crawling ItViec. Collected {len(self.jobs)} job descriptions!',
                                       '', '',
                                       f'{self.error_count}')
            else:
                _send_telegram_message('', f'Finished crawling ItViec. Collected {len(self.jobs)} job descriptions!',
                                       '', '', '')
        else:
            self.logger.info("No new URLs to process.")
            return self.jobs
        return self.jobs

    def _result_crawl_url(self, page_ranges):
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._crawl_range, start, end): (start, end)
                for start, end in page_ranges
            }

            for future in as_completed(futures):
                start, end = futures[future]
                try:
                    result = future.result()
                    if result:
                        with self.lock:
                            self.urls.extend(result)
                    self.logger.info(f"[INFO] Crawled {start}-{end}, found {len(result) if result else 0} URLs")
                except Exception as e:
                    self.logger.error(f"[ERROR] L·ªói khi crawl {start}-{end}: {e}")
                    return []

            return self.urls

    def _crawl_range(self, start_page, end_page):
        '''Crawl danh s√°ch jobs t·ª´ trang start_page ƒë·∫øn end_page s·ª≠ d·ª•ng m·ªôt driver ri√™ng.'''
        crawl_urls = []

        driver = None
        try:
            driver = self.driver_pool.get_driver()

            for page in range(start_page, end_page + 1):
                try:
                    page_url = f'https://itviec.com/it-jobs?page={page}'
                    driver.get(page_url)

                    _wait_for_element_with_driver(
                        driver,
                        By.XPATH,
                        "//div[contains(@class, 'job-card') or contains(@class, 'job_content')]",
                        self.logger
                    )

                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    job_cards = soup.find_all('div', class_='job-card')

                    jobs_on_page = []
                    for job in job_cards:
                        title_tag = job.find('h3')
                        if not title_tag:
                            continue

                        url = title_tag.get('data-url')
                        if not url:
                            continue

                        jobs_on_page.append({
                            'title': title_tag.text.strip(),
                            'url': url.strip()
                        })

                    crawl_urls.extend(jobs_on_page)
                    self.logger.info(f"[{start_page}-{end_page}] ‚úÖ Page {page}: {len(jobs_on_page)} jobs found")

                except Exception as e:
                    self.logger.error(f"[{start_page}-{end_page}] ‚ö†Ô∏è Error on page {page}: {str(e)}")

        except Exception as e:
            self.logger.error(f"‚ùå Error initializing Chrome for pages {start_page}-{end_page}: {str(e)}")

        finally:
            if driver:
                driver.quit()

        return crawl_urls

    def _fetch_job_description(self, url_obj):
        self.logger.info(f"Fetching description for: {url_obj['title']}")

        driver = self.driver_pool.get_driver()
        try:
            driver.get(url_obj['url'])

            _wait_for_element_with_driver(
                driver,
                By.XPATH,
                f"//div[contains(@class, 'jd-main')]//div[contains(@class, 'icontainer')]//h1[contains(text(), '{url_obj['title']}')]",
                self.logger
            )

            soup = BeautifulSoup(driver.page_source, 'html.parser')

            # --- Parse th√¥ng tin c∆° b·∫£n ---
            job_header = soup.find('div', class_='job-header-info')
            job_title = job_header.find('h1').text.strip()
            company_name = job_header.find('div', class_='employer-name').text.strip()
            salary = job_header.find('a').text.strip()

            job_mid = job_header.find_parent('div', class_='job-show-header').find_next_sibling()
            location_spans = job_mid.find_all('span')
            locations_text = [span.text.strip() for span in location_spans if span.text.strip()]
            posted_at = locations_text[-1] if locations_text else ''
            location = locations_text[:-1] if len(locations_text) > 1 else []

            # --- Parse k·ªπ nƒÉng v√† kinh nghi·ªám ---
            headings = ['Skills:', 'Job Domain:', 'K·ªπ nƒÉng:', 'Lƒ©nh v·ª±c:']
            experience_headings = ['Job Expertise:', 'Chuy√™n m√¥n:']
            tags, experience = [], []

            overview_divs = job_mid.find_all('div')
            for div in overview_divs:
                text = div.get_text(strip=True)
                if text in headings:
                    next_div = div.find_next_sibling("div")
                    if next_div:
                        tag_elements = next_div.find_all('a')
                        if tag_elements:
                            tags.extend(tag.text.strip() for tag in tag_elements)
                        else:
                            tags.extend(
                                span.get_text(strip=True)
                                for span in next_div.find_all('div')
                            )

                if text in experience_headings:
                    next_div = div.find_next_sibling("div")
                    if next_div:
                        experience_elems = next_div.find_all('a')
                        experience.extend(exp.text.strip() for exp in experience_elems)

            tags = _remove_duplicates(tags)
            experience = _remove_duplicates(experience)
            level = ''  # Ch∆∞a ph√¢n t√≠ch ƒë∆∞·ª£c level

            description_section = soup.find('section', class_='job-content')
            description = description_section.text.strip() if description_section else ''

            job = JobItem(
                title=job_title,
                company=company_name,
                location=location,
                salary=salary,
                posted_at=posted_at,
                experience=experience,
                level=level,
                tags=tags,
                url=url_obj['url'],
                source=self.base_url,
                description=description
            )

            self.logger.info(f"Crawled: {job_title}")
            return job

        except Exception as e:
            self.logger.error(f"‚ùå Error while crawling {job_title} - {str(e)}")
            return None

        finally:
            driver.quit()

    def _fetch_job_description_with_retry(self, url_obj, retries=3, delay=2):
        """H√†m x·ª≠ l√Ω job v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i khi c√≥ l·ªói."""
        attempt = 0
        while attempt < retries:
            try:
                result = self._fetch_job_description(url_obj)
                return result
            except Exception as e:
                attempt += 1
                self.logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω {url_obj['url']} (Th·ª≠ l·∫°i {attempt}/{retries}): {str(e)}")
                if attempt < retries:
                    time.sleep(delay)  # Ch·ªù m·ªôt ch√∫t tr∆∞·ªõc khi th·ª≠ l·∫°i
                else:
                    self.error_count += 1
                    return None  # N·∫øu ƒë√£ th·ª≠ h·∫øt c√°c l·∫ßn m√† v·∫´n kh√¥ng ƒë∆∞·ª£c, tr·∫£ v·ªÅ None
        return None
